# Twitter_Data_Transformation_Pandas_Snowflake
Dowwnloaded Dataset from kaggle, stored the file in S3 bucket, applied extensive transformations using pandas by retrieving data from S3 and removing nulls, duplicates and unnecessary columns and loading clean file to S3 Bucket, created snowpipe to load and store this cleansed data into  our snowflake table.
Have used Boto3 library to connect to S3 Bucket and loaded data from S3 Bucket to a dataframe and then apply transformations like removing columns, duplicates, nulls and then using StringIO instance we convert this transformed data into a csv file and then retrieve the file data using getvalue() function on the StringIO() instance and then load this data S3 bucket.
Now we have created snowpipe using a stage which is integrated to the S3 Bucket to load the file_data into the snowflake table.
We have installed the airflow server in the Ec2 instance and created a DAG where the task is written in PythonOperator which calls the function(the Pipeline Script which we have written to transform the twitter data).
We trigger this DAG such that the pipeline script gets executed and the script gets stored in the required S3 location provided in the script, post this a notification will be sent to the snowpipe which gets triggered as soon as the file is inserted into the S3 bucket and the transformed data will be loaded into the Table.
